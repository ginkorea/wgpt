# models.yaml
defaults:
  llm:
    temperature: 0.7
    top_p: 0.95
    max_tokens: 1024
    timeout: 300

models:
  - name: scout17b
    display_name: "Llama 4 Scout 17B Instruct (Q4_K_XL)"
    type: openai
    openai:
      base_url: "http://127.0.0.1:8080/v1"   # llama-server OpenAI-compatible endpoint
      api_key: ""                            # empty = no Authorization header sent
      model: "llama-4-scout"                 # free-form label forwarded to server
    runtime:
      kind: llama_cpp_server
      bin: "llama.cpp/build-static/bin/llama-server"
      host: "127.0.0.1"
      port: 8080
      args:
        - "--host"
        - "127.0.0.1"
        - "--port"
        - "8080"
        - "-m"
        - "models/llama/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL-00001-of-00002.gguf"
        - "-c"
        - "8192"
        - "-ngl"
        - "17"
        - "-t"
        - "0"
        - "-fa"          # was --flash-attn (invalid without value)
        - "auto"
